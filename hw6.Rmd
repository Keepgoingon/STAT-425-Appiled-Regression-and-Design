---
title: "STAT425_HW6_Jinran Yang"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

1.
(1)Forward selection based on F-test statistics
```{r}
#load data
misner<- read.csv("~/Desktop/hw6/misner(1).dat", sep="")
#forwad selection
mod<-lm(yield~1,data = misner)
indep.var<- ~ year+I(year^2)+rain+I(rain^2)+year*rain
add1(mod,indep.var,test = 'F')
mod<-update(mod, .~. +rain)#add rain
add1(mod,indep.var,test = 'F')
mod<-update(mod, .~. + year)#add year
add1(mod,indep.var,test = 'F')
mod<-update(mod, .~. + year*rain)#add interaction
add1(mod,indep.var,test = 'F')
mod<-update(mod, .~. + I(rain^2))#add rain^2
add1(mod,indep.var,test = 'F')
summary(mod)
```
Therefore, best model select by forward selection based on F-statistics is `yield ~ rain + year + I(rain^2) + rain:year`.
  
(2)Backward selection based on AIC   
```{r}
mod_b<-step(lm(yield ~ rain+year+I(year^2)+ I(rain^2)+rain*year, data = misner),k=2,direction = 'backward',trace=0)
summary(mod_b)
```
Therefore, best model select by backward selection based on AIC is `yield ~ rain + year + I(rain^2) + rain:year`, which is the same as the result of forward selection based on F-statistics. 
  
  
2.
```{r}
library(faraway)
data("seatpos")
mod1=lm(hipcenter~.,data = seatpos)
par(mfrow=c(2,2))
plot(mod1)
```
We can see a quadratic trend of residuals in the `scale-location` plot. We might need to trandform the response.
```{r}
min(seatpos$hipcenter)
seatpos$hipcenter<-seatpos$hipcenter+280
library(MASS)
mod1=lm(hipcenter~.,data = seatpos)
boxcox(mod1)
```
  
Since 1 is inside the 95% CI of lambda, it seems there is no need to trandform the response. Because there is quadtic trend in the `Residuals vs Fitted` plot, I add some quadtic terms and then perform model selection to find a small(sparse) model.
```{r}
seatpos$hipcenter<-seatpos$hipcenter-280

mod2=lm(hipcenter~.+I(Age^2) + I(Weight^2) + I(HtShoes^2) +I(Ht^2) + I(Seated^2) + I(Arm^2)
        + I(Thigh^2) +I(Leg^2),data = seatpos)
stepmode<-step(mod2,k=2,direction = "both",trace = 0 )#stepwise selection based on AIC
summary(stepmode)
par(mfrow=c(2,2))
plot(stepmode)
```
Therefore, the model selected by stepwise selection based on AIC is `hipcenter ~ Age + HtShoes + Leg + I(Age^2) + I(HtShoes^2)` which can explain 71.64% variation of `hipcenter`. According to the diagnostics plots, all plots looks good, and `Scale-Location` plot is more flatter than before.

```{r}
nrow(seatpos)
forwardmode=step(mod2,k=log(38),direction = "forward" )
summary(forwardmode)#too many vairables, not good
```

4.
```{r}
library(MASS)
beta=vector(length = 20)
beta[1:3]=c(1,-1,1)
beta[4:20]=0
I<-diag(x=1,nrow = 20,ncol = 20)
J<- matrix(1,20,20)
sigma<-0.7*I + 0.3*J
mean<-matrix(0,100,1)

set.seed(1)
X=mvrnorm(100, mu=rep(0,20), Sigma=sigma) 
error<-rnorm(100, mean = 0, sd = 1)
Y=X%*%beta+error
##least square
ls<-lm(Y~X)
mse_ls<-sum((ls$coefficients[-1]-beta)^2) 
mse_ls
#ridge
library(glmnet)
cv.ridge<- cv.glmnet(X,Y, alpha =0)
model.ridge<- glmnet(X,Y, lambda = cv.ridge$lambda.min,alpha = 0)
mse_ridge<-sum((model.ridge$beta-beta)^2) 
mse_ridge
#lasso
cv.lasso<- cv.glmnet(X,Y, alpha =1)
model.lasso<- glmnet(X,Y, lambda = cv.lasso$lambda.min,alpha = 1)
mse_lasso<-sum((model.lasso$beta-beta)^2) 
mse_lasso
#100 synthetic datasets and average the values of the MSEs
n=100
p=20
time=100 
mse_ls=mse_ridge=mse_lasso=rep(0,time) 
set.seed(1)

for(i in 1:time)
{
X=mvrnorm(n, mu=rep(0,p), Sigma=sigma) 
error<-rnorm(100, mean = 0, sd = 1)
Y=X%*%beta+error
ls<-lm(Y~X)
mse_ls[i]<-sum((ls$coefficients[-1]-beta)^2) 

cv.ridge<- cv.glmnet(X,Y, alpha =0)
model.ridge<- glmnet(X,Y, lambda = cv.ridge$lambda.min,alpha = 0)
mse_ridge[i]<-sum((model.ridge$beta-beta)^2)


cv.lasso<- cv.glmnet(X,Y, alpha =1)
model.lasso<- glmnet(X,Y, lambda = cv.lasso$lambda.min,alpha = 1)
mse_lasso[i]<-sum((model.lasso$beta-beta)^2)
}
mean(mse_ls)
mean(mse_ridge)
mean(mse_lasso)
```
As we can see the performance of least squares is the worst, and the performance of lasso regression is the best. The performance of ridge is slightly better than least squares.










